import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import joblib
import random
from datetime import datetime, timedelta

# å¼•å…¥ ONNX è½¬æ¢åº“
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType

# ==========================================
# 1. æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ (åŸºäºè®ºæ–‡ Source 113-117)
# ==========================================
def generate_mock_data(n_samples=5000):
    data = []
    base_time = datetime.now()
    
    for _ in range(n_samples):
        # æ¨¡æ‹Ÿï¼š90% æ˜¯æ­£å¸¸æ“ä½œï¼Œ10% æ˜¯å¼‚å¸¸
        is_anomaly = random.random() < 0.1
        
        # 1. æ—¶é—´ç‰¹å¾
        # æ­£å¸¸æ“ä½œå¤šåœ¨å·¥ä½œæ—¶é—´ (9-18)ï¼Œå¼‚å¸¸å¯èƒ½åœ¨æ·±å¤œ
        if is_anomaly and random.random() > 0.5:
            dt = base_time - timedelta(hours=random.randint(0, 24))
        else:
            # å€¾å‘äºå·¥ä½œæ—¶é—´
            dt = base_time.replace(hour=random.randint(9, 18))
            
        # 2. é‡çº§ç‰¹å¾
        # å¼‚å¸¸æ“ä½œå¾€å¾€æ¶‰åŠå¤§é‡è¡Œæ•° (å¦‚æ‹–åº“)
        if is_anomaly:
            row_count = random.randint(1000, 1000000)
            exec_time = random.randint(1000, 60000) # ms
        else:
            row_count = random.randint(0, 100)
            exec_time = random.randint(1, 500)

        # 3. ç»“æ„ç‰¹å¾
        # å¤æ‚çš„å…³è”æŸ¥è¯¢æˆ–è¶…é•¿ SQL
        if is_anomaly:
            sql_len = random.randint(500, 5000)
            num_tables = random.randint(3, 10)
            num_joins = random.randint(2, 8)
        else:
            sql_len = random.randint(20, 200)
            num_tables = 1
            num_joins = 0

        # 4. é¢‘ç‡ç‰¹å¾
        # è¿‡å»1åˆ†é’Ÿå†…çš„è°ƒç”¨æ¬¡æ•° (éœ€åœ¨Javaä¾§èšåˆï¼Œè¿™é‡Œæ¨¡æ‹Ÿæ•°å€¼)
        freq_1min = random.randint(50, 200) if is_anomaly else random.randint(0, 10)

        data.append({
            'timestamp': dt,
            'row_count': row_count,
            'exec_time': exec_time,
            'sql_length': sql_len,
            'num_tables': num_tables,
            'num_joins': num_joins,
            'freq_1min': freq_1min
        })
    
    return pd.DataFrame(data)

# ==========================================
# 2. ç‰¹å¾å·¥ç¨‹å¤„ç†
# ==========================================
def preprocess_features(df):
    # æå– hour_of_day
    df['hour_of_day'] = df['timestamp'].dt.hour
    
    # æå– is_workday (0/1)
    # ç®€å•é€»è¾‘ï¼šå‘¨ä¸€(0)åˆ°å‘¨äº”(4)ä¸ºå·¥ä½œæ—¥
    df['is_workday'] = df['timestamp'].dt.dayofweek.apply(lambda x: 1 if x < 5 else 0)
    
    # é‡çº§ç‰¹å¾å¯¹æ•°åŒ–å¤„ç†
    # ä½¿ç”¨ log1p é¿å… log(0)
    df['log_row_count'] = np.log1p(df['row_count'])
    df['log_exec_time'] = np.log1p(df['exec_time'])
    
    # é€‰å®šæœ€ç»ˆç”¨äºè®­ç»ƒçš„ç‰¹å¾å‘é‡
    feature_cols = [
        'hour_of_day', 'is_workday',
        'log_row_count', 'log_exec_time',
        'sql_length', 'num_tables', 'num_joins',
        'freq_1min'
    ]
    return df[feature_cols]

# ==========================================
# 3. æ¨¡å‹è®­ç»ƒä¸ä¿å­˜
# ==========================================
def train_and_save():
    print("Generating mock data...")
    raw_df = generate_mock_data()

    print("Preprocessing features...")
    X = preprocess_features(raw_df)

    # å­¤ç«‹æ£®æ—å‚æ•°å»ºè®®
    clf = IsolationForest(
        n_estimators=100,
        max_samples='auto',
        contamination=0.05,
        random_state=42,
        n_jobs=-1
    )

    model_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('iso_forest', clf)
    ])

    print("Training Isolation Forest model...")
    model_pipeline.fit(X)

    # ä¿å­˜ Joblib æ¨¡å‹ (Python ä½¿ç”¨)
    joblib.dump(model_pipeline, 'db_audit_iso_forest.joblib')

    # ==========================================
    # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šå¯¼å‡ºä¸º ONNX
    # ==========================================
    print("Converting to ONNX...")

    # 1. å®šä¹‰è¾“å…¥ç±»å‹
    n_features = 8
    initial_type = [('float_input', FloatTensorType([None, n_features]))]

    # 2. è½¬æ¢ (ä¿®å¤äº†æŠ¥é”™çš„éƒ¨åˆ†)
    # target_opset={'': 12, 'ai.onnx.ml': 3}
    # '': 12 ä»£è¡¨é€šç”¨ç®—å­(Add, MatMulç­‰)ä½¿ç”¨ç‰ˆæœ¬ 12
    # 'ai.onnx.ml': 3 ä»£è¡¨æœºå™¨å­¦ä¹ ç®—å­(TreeEnsembleç­‰)å¼ºåˆ¶ä½¿ç”¨ç‰ˆæœ¬ 3
    onnx_model = convert_sklearn(
        model_pipeline,
        initial_types=initial_type,
        target_opset={'': 12, 'ai.onnx.ml': 3}
    )

    # 3. ä¿å­˜
    onnx_path = 'deep_audit_iso_forest.onnx'
    with open(onnx_path, "wb") as f:
        f.write(onnx_model.SerializeToString())

    print(f"ONNX Model saved to {onnx_path}")

if __name__ == "__main__":
    train_and_save()
