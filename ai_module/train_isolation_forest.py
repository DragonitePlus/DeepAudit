import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import joblib
import json
import random
import re
from datetime import datetime, timedelta
from sqlalchemy import create_engine
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType

# ==========================================
# 0. é…ç½®éƒ¨åˆ† (Configuration)
# ==========================================
# æ•°æ®åº“è¿æ¥ï¼Œç”¨äºæ‹‰å–çœŸå®åé¦ˆæ•°æ® (Feedback Loop)
DB_CONNECTION_STR = "mysql+pymysql://root:root@127.0.0.1:3306/deepaudit_sys"

# è®­ç»ƒæ ·æœ¬è§„æ¨¡ (Large Scale)
N_SAMPLES = 100000

# ==========================================
# 1. ç§‘å­¦æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ (Scientific Mock Data)
# ==========================================
def generate_mock_data(n_samples=50000):
    """
    ç”Ÿæˆç¬¦åˆçœŸå®æ•°æ®åº“è´Ÿè½½åˆ†å¸ƒçš„æ¨¡æ‹Ÿæ•°æ®ã€‚
    åŒ…å« 95% æ­£å¸¸æµé‡ + 5% æ”»å‡»æµé‡ (è¦†ç›– 5 å¤§æ”»å‡»åœºæ™¯)ã€‚
    """
    print(f"DeepAudit: Generating {n_samples} samples with comprehensive attack scenarios...")

    n_normal = int(n_samples * 0.95)
    n_anomaly = n_samples - n_normal
    base_time = datetime.now()

    data = []

    # -------------------------------------------------------------------------
    # A. æ­£å¸¸æµé‡ (Normal Behavior - 95%)
    # ç‰¹å¾ï¼šä½å¤æ‚åº¦ã€å·¥ä½œæ—¶é—´ã€æ— é”™è¯¯ã€æ ‡å‡†å®¢æˆ·ç«¯
    # -------------------------------------------------------------------------
    # 1. æ—¶é—´åˆ†å¸ƒï¼šé«˜æ–¯åˆ†å¸ƒï¼Œé›†ä¸­åœ¨ 14:00 (sigma=3h)
    hours_normal = np.random.normal(loc=14, scale=3, size=n_normal)
    hours_normal = np.clip(hours_normal, 0, 23).astype(int)

    # 2. æŸ¥è¯¢è¡Œæ•° (SELECT)ï¼šå¯¹æ•°æ­£æ€åˆ†å¸ƒ (å¤§éƒ¨åˆ†æŸ¥è¯¢åªè¿”å›å‡ è¡Œ)
    rows_normal = np.random.lognormal(mean=2, sigma=1.2, size=n_normal).astype(int)

    # 3. å½±å“è¡Œæ•° (DML)ï¼š90% ä¸º 0 (è¯»æ“ä½œ)ï¼Œ10% ä¸ºå†™æ“ä½œ (å°‘é‡è¡Œ)
    is_write = np.random.random(n_normal) < 0.1
    affected_normal = np.zeros(n_normal)
    affected_normal[is_write] = np.random.lognormal(mean=0.5, sigma=0.8, size=np.sum(is_write))

    # 4. AST ç»“æ„ç‰¹å¾ (æ­£å¸¸ SQL é€šå¸¸å¾ˆç®€å•)
    # condition_count: WHERE æ¡ä»¶æ•° (æ³Šæ¾åˆ†å¸ƒ, lambda=1)
    cond_count_normal = np.random.poisson(lam=1, size=n_normal)
    # join_count: è¿æ¥è¡¨æ•° (å¤§éƒ¨åˆ†ä¸º 0 æˆ– 1)
    join_count_normal = np.random.choice([0, 1, 2], size=n_normal, p=[0.7, 0.25, 0.05])
    # nested_level: åµŒå¥—å±‚çº§ (0)
    nested_normal = np.zeros(n_normal)
    # has_always_true: æ˜¯å¦åŒ…å« 1=1 (ç»å¯¹æ²¡æœ‰)
    always_true_normal = np.zeros(n_normal)

    # 5. å®¢æˆ·ç«¯ç‰¹å¾ (0=æ­£å¸¸å®¢æˆ·ç«¯, 1=å¼‚å¸¸è„šæœ¬)
    client_risk_normal = np.zeros(n_normal)

    # 6. è€—æ—¶ & é¢‘ç‡
    exec_time_normal = np.random.lognormal(mean=3, sigma=1.0, size=n_normal).astype(int) # ms
    freq_normal = np.random.poisson(lam=5, size=n_normal)

    # 7. é”™è¯¯ç  (0=æˆåŠŸ)
    error_risk_normal = np.zeros(n_normal)

    # ç”Ÿæˆæ­£å¸¸æ•°æ®é›†
    for i in range(n_normal):
        dt = base_time - timedelta(days=random.randint(0, 7))
        dt = dt.replace(hour=hours_normal[i], minute=random.randint(0, 59))

        # SQL ç±»å‹æƒé‡: å†™æ“ä½œ=3, è¯»æ“ä½œ=1
        w = 3 if is_write[i] else 1

        data.append({
            'timestamp': dt,
            'row_count': rows_normal[i],
            'affected_rows': int(affected_normal[i]),
            'exec_time': exec_time_normal[i],
            'sql_type_weight': w,
            'freq_1min': freq_normal[i],
            # AST ç‰¹å¾
            'condition_count': cond_count_normal[i],
            'join_count': join_count_normal[i],
            'nested_level': int(nested_normal[i]),
            'has_always_true': int(always_true_normal[i]),
            # ç¯å¢ƒç‰¹å¾
            'client_app_risk': int(client_risk_normal[i]),
            'error_code_risk': int(error_risk_normal[i]),
            'label': 0
        })

    # -------------------------------------------------------------------------
    # B. å¼‚å¸¸æµé‡ (Anomaly Scenarios - 5%)
    # æ¨¡æ‹ŸçœŸå®æ”»å‡»åœºæ™¯ï¼Œè®­ç»ƒæ¨¡å‹è¯†åˆ«è¿™äº›æ¨¡å¼
    # -------------------------------------------------------------------------

    # åœºæ™¯1: SQL æ³¨å…¥ (Boolean Injection)
    # ç‰¹å¾: åŒ…å« 1=1, é«˜æ¡ä»¶æ•°, å¯èƒ½æœ‰é”™è¯¯
    for _ in range(int(n_anomaly * 0.2)):
        data.append({
            'timestamp': base_time,
            'row_count': random.randint(0, 100),
            'affected_rows': 0,
            'exec_time': random.randint(10, 200),
            'sql_type_weight': 1,
            'freq_1min': random.randint(10, 50),
            # AST å¼‚å¸¸
            'condition_count': random.randint(5, 10), # å¤æ‚æ¡ä»¶
            'join_count': 0,
            'nested_level': 0,
            'has_always_true': 1, # ğŸ”¥ è‡´å‘½ç‰¹å¾ (1=1)
            'client_app_risk': 1, # å¯èƒ½ä½¿ç”¨ sqlmap
            'error_code_risk': random.choice([0, 1]),
            'label': 1
        })

    # åœºæ™¯2: æ‹–åº“ (Data Exfiltration)
    # ç‰¹å¾: å·¨å¤§ row_count, æ·±å¤œè®¿é—®
    for _ in range(int(n_anomaly * 0.2)):
        data.append({
            'timestamp': base_time.replace(hour=3), # æ·±å¤œ
            'row_count': np.random.randint(50000, 1000000), # ğŸ”¥ æ‹–åº“
            'affected_rows': 0,
            'exec_time': np.random.randint(5000, 60000),
            'sql_type_weight': 1,
            'freq_1min': random.randint(1, 5),
            'condition_count': 1,
            'join_count': 0,
            'nested_level': 0,
            'has_always_true': 0,
            'client_app_risk': 0,
            'error_code_risk': 0,
            'label': 1
        })

    # åœºæ™¯3: æ¶æ„åˆ æ”¹ (Destructive Operation)
    # ç‰¹å¾: å·¨å¤§ affected_rows, é«˜æƒé‡ç±»å‹(DDL/DML)
    for _ in range(int(n_anomaly * 0.2)):
        data.append({
            'timestamp': base_time,
            'row_count': 0,
            'affected_rows': np.random.randint(1000, 50000), # ğŸ”¥ åˆ åº“
            'exec_time': np.random.randint(1000, 10000),
            'sql_type_weight': 5, # DDL/é«˜å±
            'freq_1min': random.randint(1, 5),
            'condition_count': 1,
            'join_count': 0,
            'nested_level': 0,
            'has_always_true': 0,
            'client_app_risk': 0,
            'error_code_risk': 0,
            'label': 1
        })

    # åœºæ™¯4: æ…¢æŸ¥è¯¢ DoS (Denial of Service)
    # ç‰¹å¾: æé«˜ exec_time, é«˜ join_count, é«˜åµŒå¥—
    for _ in range(int(n_anomaly * 0.2)):
        data.append({
            'timestamp': base_time,
            'row_count': 100,
            'affected_rows': 0,
            'exec_time': np.random.randint(30000, 100000), # ğŸ”¥ 30s+
            'sql_type_weight': 1,
            'freq_1min': random.randint(1, 5),
            'condition_count': random.randint(5, 20),
            'join_count': random.randint(5, 10), # ğŸ”¥ å¤šè¡¨å…³è”
            'nested_level': random.randint(2, 5), # ğŸ”¥ åµŒå¥—æŸ¥è¯¢
            'has_always_true': 0,
            'client_app_risk': 0,
            'error_code_risk': 0,
            'label': 1
        })

    # åœºæ™¯5: æš´åŠ›æ¢æµ‹ (Brute Force / Scanning)
    # ç‰¹å¾: é«˜é¢‘, é«˜é”™è¯¯ç‡, è„šæœ¬å®¢æˆ·ç«¯
    for _ in range(int(n_anomaly * 0.2)):
        data.append({
            'timestamp': base_time,
            'row_count': 0,
            'affected_rows': 0,
            'exec_time': random.randint(1, 10),
            'sql_type_weight': 1,
            'freq_1min': np.random.randint(100, 500), # ğŸ”¥ æé«˜é¢‘
            'condition_count': 0,
            'join_count': 0,
            'nested_level': 0,
            'has_always_true': 0,
            'client_app_risk': 1, # ğŸ”¥ è„šæœ¬å·¥å…·
            'error_code_risk': 1, # ğŸ”¥ é¢‘ç¹æŠ¥é”™
            'label': 1
        })

    df = pd.DataFrame(data)
    # æ··æ´—æ•°æ®
    df = df.sample(frac=1).reset_index(drop=True)
    return df

# ==========================================
# 2. ä»æ•°æ®åº“æå–çœŸå®æ•°æ® (ETL)
# ==========================================
def extract_sql_features_simple(sql):
    """ç®€å•çš„ SQL ç‰¹å¾æå– (ç”¨äºä»æ•°æ®åº“æ¢å¤ç‰¹å¾)"""
    if not sql: return 0, 0, 0, 0
    s = str(sql).lower()
    cond = s.count(' and ') + s.count(' or ') + s.count(' where ')
    join = s.count(' join ')
    nested = s.count(' select ') - 1
    injection = 1 if '1=1' in s or '1 = 1' in s else 0
    return cond, join, max(0, nested), injection

def fetch_real_data_from_db():
    print("DeepAudit: Connecting to database to fetch real feedback data...")
    try:
        engine = create_engine(DB_CONNECTION_STR)

        # åªè¯»å–äººå·¥æ ‡è®°ä¸º"æ­£å¸¸"(feedback_status=1)çš„æ•°æ®ä½œä¸ºæ­£æ ·æœ¬
        query = """
        SELECT create_time, result_count, affected_rows, execution_time, 
               error_code, sql_template, app_user_id, client_app, action_taken
        FROM sys_audit_log
        WHERE feedback_status = 1
        LIMIT 10000
        """
        df = pd.read_sql(query, engine)

        if df.empty:
            print("DeepAudit: No real feedback data found. Using mock data only.")
            return pd.DataFrame()

        print(f"DeepAudit: Fetched {len(df)} rows of real feedback data.")

        # å­—æ®µæ˜ å°„ä¸å¡«å……
        df['timestamp'] = pd.to_datetime(df['create_time'])
        df['row_count'] = df['result_count'].fillna(0)
        df['affected_rows'] = df['affected_rows'].fillna(0)
        df['exec_time'] = df['execution_time'].fillna(0)

        # è§£æ SQL ç‰¹å¾ (æ¨¡æ‹Ÿ Java ç«¯çš„ AST è§£æ)
        feats = df['sql_template'].apply(extract_sql_features_simple)
        df['condition_count'] = [x[0] for x in feats]
        df['join_count'] = [x[1] for x in feats]
        df['nested_level'] = [x[2] for x in feats]
        df['has_always_true'] = [x[3] for x in feats]

        # è§£æå®¢æˆ·ç«¯é£é™© (ç®€å•è§„åˆ™: python/curl/sqlmap è§†ä¸ºé«˜å±)
        def get_client_risk(app):
            app = str(app).lower()
            if 'python' in app or 'curl' in app or 'sqlmap' in app: return 1
            return 0
        df['client_app_risk'] = df['client_app'].apply(get_client_risk)

        # é”™è¯¯ç é£é™©
        df['error_code_risk'] = df['error_code'].apply(lambda x: 1 if x and x > 0 else 0)

        # SQL ç±»å‹æƒé‡
        def get_type_weight(sql):
            s = str(sql).lower()
            if 'drop ' in s or 'truncate ' in s or 'grant ' in s: return 5
            if 'update ' in s or 'delete ' in s or 'insert ' in s: return 3
            return 1
        df['sql_type_weight'] = df['sql_template'].apply(get_type_weight)

        # è®¡ç®— freq_1min
        df = df.sort_values('timestamp')
        freq = df.set_index('timestamp').groupby('app_user_id').rolling('1min')['sql_template'].count().reset_index()
        df['freq_1min'] = freq['sql_template'].values

        df['label'] = 0
        return df
    except Exception as e:
        print(f"DeepAudit DB Error: {e}")
        return pd.DataFrame()

# ==========================================
# 3. ç‰¹å¾å·¥ç¨‹å¤„ç† (Feature Engineering)
# ==========================================
def preprocess_features(df):
    if df.empty: return df

    # 1. æ—¶é—´ç‰¹å¾
    df['hour_of_day'] = df['timestamp'].dt.hour
    df['is_workday'] = df['timestamp'].dt.dayofweek.apply(lambda x: 1 if x < 5 else 0)

    # 2. é‡çº§ç‰¹å¾å¯¹æ•°åŒ– (Log Transform)
    df['log_row_count'] = np.log1p(df['row_count'])
    df['log_affected_rows'] = np.log1p(df['affected_rows'])
    df['log_exec_time'] = np.log1p(df['exec_time'])

    # 3. é€‰å®šæœ€ç»ˆç‰¹å¾åˆ— (å…± 13 ä¸ªç‰¹å¾ï¼Œä¸ Java FeatureExtractor å¿…é¡»ä¸€è‡´)
    feature_cols = [
        'hour_of_day',
        'is_workday',
        'log_row_count',
        'log_affected_rows',
        'log_exec_time',
        'freq_1min',
        'sql_type_weight',
        # AST ç‰¹å¾
        'condition_count',
        'join_count',
        'nested_level',
        'has_always_true',
        # ç¯å¢ƒç‰¹å¾
        'client_app_risk',
        'error_code_risk'
    ]

    return df[feature_cols].fillna(0)

# ==========================================
# 4. è§£é‡Šè§„åˆ™ä¸æ‰“åˆ†ç”Ÿæˆ (Scoring Logic)
# ==========================================
def generate_explanation_rules(df_processed, model, scaler):
    """
    ç”ŸæˆåŒ…å«'æ‰£åˆ†é€»è¾‘'çš„è§£é‡Šè§„åˆ™ã€‚
    ç­–ç•¥:
    - 100åˆ†æ»¡åˆ† (Trust Score)
    - æ‰£ 40åˆ† -> è§‚å¯Ÿ (Observation)
    - æ‰£ 100åˆ† -> é˜»æ–­ (Block)
    """
    print("DeepAudit: Generating scoring rules based on normal distribution...")

    feature_names = df_processed.columns.tolist()

    # é¢„æµ‹å¹¶ç­›é€‰æ­£å¸¸æ ·æœ¬ (åŸºå‡†çº¿)
    X_scaled = scaler.transform(df_processed)
    preds = model.predict(X_scaled)
    normal_df = df_processed[preds == 1]

    rules = {}

    # å®šä¹‰æ¯ä¸ªç‰¹å¾çš„æ‰£åˆ†ç­–ç•¥ (Risk Deduction)
    # deduction: åŸºç¡€æ‰£åˆ†å€¼
    # critical: æ˜¯å¦ç›´æ¥è‡´å‘½ (true = å•æ¬¡æ‰£100)
    meta = {
        'hour_of_day':       {'desc': "éå·¥ä½œæ—¶é—´è®¿é—®", 'deduction': 20, 'critical': False},
        'is_workday':        {'desc': "éå·¥ä½œæ—¥è®¿é—®", 'deduction': 10, 'critical': False},
        'log_row_count':     {'desc': "è¿”å›è¡Œæ•°å¼‚å¸¸(ç–‘ä¼¼æ‹–åº“)", 'deduction': 50, 'critical': False},
        'log_affected_rows': {'desc': "å½±å“è¡Œæ•°å¼‚å¸¸(ç–‘ä¼¼åˆ æ”¹)", 'deduction': 60, 'critical': False},
        'log_exec_time':     {'desc': "æ‰§è¡Œè€—æ—¶è¿‡é•¿(æ…¢æŸ¥è¯¢)", 'deduction': 30, 'critical': False},
        'freq_1min':         {'desc': "é«˜é¢‘è®¿é—®(ç–‘ä¼¼æ‰«æ)", 'deduction': 20, 'critical': False},
        'sql_type_weight':   {'desc': "é«˜é£é™©SQLç±»å‹", 'deduction': 30, 'critical': False},
        'condition_count':   {'desc': "SQLæ¡ä»¶è¿‡äºå¤æ‚", 'deduction': 40, 'critical': False},
        'join_count':        {'desc': "å¤šè¡¨å…³è”å¼‚å¸¸", 'deduction': 40, 'critical': False},
        'nested_level':      {'desc': "åµŒå¥—å±‚çº§è¿‡æ·±", 'deduction': 30, 'critical': False},
        'has_always_true':   {'desc': "SQLæ³¨å…¥ç‰¹å¾(1=1)", 'deduction': 100, 'critical': True}, # ğŸ”¥ å¿…æ€
        'client_app_risk':   {'desc': "éæ³•å®¢æˆ·ç«¯å·¥å…·", 'deduction': 80, 'critical': False},
        'error_code_risk':   {'desc': "æ•°æ®åº“å¼‚å¸¸æŠ¥é”™", 'deduction': 20, 'critical': False}
    }

    for col in feature_names:
        # ç»Ÿè®¡æ­£å¸¸æ•°æ®çš„ 99.9% åˆ†ä½ç‚¹ä½œä¸ºé˜ˆå€¼
        upper = normal_df[col].quantile(0.999)
        lower = normal_df[col].quantile(0.001)

        # ç‰¹æ®Šå¤„ç†å¸ƒå°”å€¼ç‰¹å¾ (å¦‚ has_always_true)
        if col in ['has_always_true', 'client_app_risk', 'error_code_risk']:
            upper = 0.5 # åªè¦æ˜¯ 1 å°±æ˜¯å¼‚å¸¸

        info = meta.get(col, {'desc': col, 'deduction': 10})

        rules[col] = {
            "desc": info['desc'],
            "max": float(upper),
            "min": float(lower),
            "deduction": info['deduction'], # å»ºè®®æ‰£åˆ†
            "is_critical": info['critical'] # æ˜¯å¦ç›´æ¥é˜»æ–­
        }

    return rules

# ==========================================
# 5. ä¸»æµç¨‹
# ==========================================
def train_and_save():
    # 1. ç”Ÿæˆå¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®
    print("--- Step 1: Generating Data ---")
    mock_df = generate_mock_data(n_samples=N_SAMPLES)

    # 2. èåˆçœŸå®åé¦ˆ (é—­ç¯è¿›åŒ–)
    print("--- Step 2: Integrating Feedback ---")
    real_df = fetch_real_data_from_db()
    if not real_df.empty:
        # è¿‡é‡‡æ ·çœŸå®ç™½åå•æ•°æ®ï¼Œå¢åŠ å…¶æƒé‡
        real_df_weighted = pd.concat([real_df] * 10, ignore_index=True)
        final_df = pd.concat([mock_df, real_df_weighted], ignore_index=True)
    else:
        final_df = mock_df

    print(f"Total training samples: {len(final_df)}")

    # 3. é¢„å¤„ç†
    print("--- Step 3: Preprocessing ---")
    X = preprocess_features(final_df)

    # 4. è®­ç»ƒ Isolation Forest
    print("--- Step 4: Training Model ---")
    clf = IsolationForest(
        n_estimators=300, # å¢åŠ æ ‘çš„æ•°é‡æå‡ç¨³å®šæ€§
        max_samples='auto',
        contamination=0.05, # é¢„è®¡å¼‚å¸¸æ¯”ä¾‹
        random_state=42,
        n_jobs=-1
    )

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    clf.fit(X_scaled)

    model_pipeline = Pipeline([
        ('scaler', scaler),
        ('iso_forest', clf)
    ])

    # 5. ç”Ÿæˆæ‰“åˆ†è§„åˆ™ (Explainability & Scoring)
    print("--- Step 5: Generating Scoring Rules ---")
    rules = generate_explanation_rules(X, clf, scaler)
    with open('model_explanation_rules.json', 'w', encoding='utf-8') as f:
        json.dump(rules, f, ensure_ascii=False, indent=2)
    print("Scoring rules saved to 'model_explanation_rules.json'")

    # 6. å¯¼å‡º ONNX æ¨¡å‹
    print("--- Step 6: Exporting ONNX ---")
    n_features = X.shape[1]
    initial_type = [('float_input', FloatTensorType([None, n_features]))]

    onnx_model = convert_sklearn(
        model_pipeline,
        initial_types=initial_type,
        target_opset={'': 12, 'ai.onnx.ml': 3}
    )

    with open('deep_audit_iso_forest.onnx', "wb") as f:
        f.write(onnx_model.SerializeToString())

    print("âœ… Model trained and saved successfully!")

if __name__ == "__main__":
    train_and_save()